---
title: "compute"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
#chunk options
knitr::opts_chunk$set(echo = TRUE)

install.packages("pacman")
library(pacman)
p_load("scales","cowplot", "randomForest", "gridExtra", "haven", "dbplyr","RSQLite", "readr", "missForest", "tictoc","tidyverse")


```


----------------------------------------------------------------------
# Loading Data and Wrangling:
----------------------------------------------------------------------

```{r}
data <- read.csv("full.csv", header = TRUE)

data <- data %>% 
    select(-c(w1_nc_unemdc, w1_nc_emwrk_isco_c)) %>% #drop vars with too many NAs, captured by two other vars
    mutate(w1_nc_emtyp = replace_na(w1_nc_emtyp, 0),  # replace nas with 0's, for unemployed 
           w1_nc_edter = replace_na(w1_nc_edter, 2))

### Assign missing values (Refused (-8), Not Applicable (-5), Don't Know (-9), Missing (-3)) as NAs. This is in order to impute missing values later.

data <- data %>% 
    mutate(w1_nc_hhincchng = ifelse(w1_nc_hhincchng < 0, NA, w1_nc_hhincchng), # hhincome change, change to NA, there are very few values. need no ,missing for this var
           w1_nc_hhinc = ifelse(w1_nc_hhinc < 0, NA, w1_nc_hhinc), # missing
           w1_nc_hhincdec1 = ifelse(w1_nc_hhincdec1 < 0, NA, w1_nc_hhincdec1), #missing
           w1_nc_incgov = ifelse(w1_nc_incgov < 0, NA, w1_nc_incgov), #missing
           w1_nc_hhincsrc1 = ifelse(w1_nc_hhincsrc1 < 0, NA, w1_nc_hhincsrc1), #missing
           w1_nc_emtyp = ifelse(w1_nc_emtyp < 0, NA, w1_nc_emtyp), #missing
           w1_nc_enrgelec = ifelse(w1_nc_enrgelec < 0, NA, w1_nc_enrgelec),#missing
           w1_nc_watsrc = ifelse(w1_nc_watsrc < 0, NA, w1_nc_watsrc),#missing
           w1_nc_nocld = ifelse(w1_nc_nocld < 0, NA, w1_nc_nocld),#missing
           w1_nc_nopres = ifelse(w1_nc_nopres < 0, NA, w1_nc_nopres),#missing
           w1_nc_dwltyp = ifelse(w1_nc_dwltyp < 0, NA, w1_nc_dwltyp), #missing
           w1_nc_moveres_apr = ifelse(w1_nc_moveres_apr < 0, NA, w1_nc_moveres_apr),
           w1_nc_prov = ifelse(w1_nc_prov < 0,NA, w1_nc_prov),
           w1_nc_prov = ifelse(w1_nc_prov == 10, NA, w1_nc_prov), # 10 = outside of south africa, 3 cases
           w1_nc_edter = ifelse(w1_nc_edter < 0, NA, w1_nc_edter),
           w1_nc_edschgrd = ifelse(w1_nc_edschgrd < 0, NA, w1_nc_edschgrd),
           w1_nc_geo2011 = ifelse(w1_nc_geo2011 < 0, NA, w1_nc_geo2011),
           w1_nc_empl_stat = ifelse(w1_nc_empl_stat < 0,NA, w1_nc_empl_stat),
           w1_nc_mdbdc2011 = ifelse(w1_nc_mdbdc2011 < 0, NA, w1_nc_mdbdc2011))

#check amount of NAs in each col
#colSums(is.na(data))

#Renaming variables:
data <- data %>% 
    rename(  "Income Change" = w1_nc_hhincchng,
             "PID" = pid,
             "HH Income Apr" = w1_nc_hhinc,
             "Sources Income Decreased" = w1_nc_hhincdec1,
             "Grant" = w1_nc_incgov,
             "Sources HH Income" = w1_nc_hhincsrc1,
             "Employment Type" = w1_nc_emtyp,
             "Electricity Access" = w1_nc_enrgelec,
             "Water Access" = w1_nc_watsrc,
             "Children Change" = w1_nc_nocld,
             "HH Size " = w1_nc_nopres,
             "Dwelling Type" = w1_nc_dwltyp,
             "Moved" = w1_nc_moveres_apr,
             "Province" = w1_nc_prov,
             "Tertiary" = w1_nc_edter,
             "Education" = w1_nc_edschgrd,
             "Age" = w1_nc_best_age_yrs,
             "Age Interval" = w1_nc_age_intervals,
             "Race" = w1_nc_best_race,
             "Employed" = w1_nc_empl_stat,
             "Geo Type" = w1_nc_geo2011,
             "Gender" = w1_nc_best_gen,
             "District Council" = w1_nc_mdbdc2011)

# making dummy variables: moved, grant, elec, water, tertiary, gender

data <- data %>% 
    mutate(Grant = ifelse(Grant == 2, 0, Grant), #yes = 1, no = 0
           Moved = ifelse(Moved == 2, 0, Moved),#yes = 1, no = 0
           `Electricity Access` = ifelse(`Electricity Access` == 2, 0, `Electricity Access`),#yes = 1, no = 0
           `Water Access` = ifelse(`Water Access` == 2, 0, `Water Access`),#yes = 1, no = 0
           Tertiary = ifelse(Tertiary == 2, 0, Tertiary),#yes = 1, no = 0
           Gender = ifelse(Gender == 2, 0, Gender)) # Male = 1, female = 0

# dropping cols and mutating data classes to factor for classification:

factor_cols <- c("Income Change", "District Council", "Sources Income Decreased", "Employed", "Employment Type", "Sources HH Income", "Children Change", "Dwelling Type", "Province","Race","Geo Type", "Water Access", "Grant", "Electricity Access", "Water Access", "Tertiary", "Gender")

data <- data %>% 
    mutate_at(factor_cols, funs(as.factor(.)))

data <- data %>% 
    select(-X, -PID)


# Splitting district council variable (with 54 levels), into separate dummies, as the randomForest algorithm only allows for 53 levels in a categorical variable.

p_load(fastDummies)
data <- dummy_cols(data, select_columns = 'District Council', remove_selected_columns = TRUE)
```

----------------------------------------------------------------------
# Imputing missing values using random forest  - missForest package:
----------------------------------------------------------------------

```{r}

## testing with a subsample:

head_impute <- head(data, n = 100)
imputed_head <- missForest(head_impute, 10, ntree = 500, mtry = 8, verbose = TRUE)

imputed_head$ximp %>% view()


#testing with parallel
p_load("doParallel", "doRNG")

head_impute <- head(data, n = 100)

doParallel::registerDoParallel(cores = 8) # set based on number of CPU cores
doRNG::registerDoRNG(seed = 123)

tic()
imputed_head <- missForest(head_impute, 20, ntree = 500, mtry = 8, verbose = TRUE, parallelize = "variables")
toc()

imputed_head$ximp %>% view()


# working, imputes missing values and retains data classes.

## entire sample:

tic()
imputed <-  missForest(data, 3, ntree = 50, mtry = 8, verbose = TRUE, parallelize = "forests")
toc()

imputed_full <- imputed$ximp

#write imputed data to csv
write.csv(imputed_full, "imputed_full.csv")


```

######################################################################
----------------------------------------------------------------------
### Random Forest on Imputed data - Classification:
----------------------------------------------------------------------
######################################################################

# Focus:

Want to compute a RF to look at the effect of Lockdown on whether households have lost their main source of income. i.e. who are most likely to have lost their main income due to lockdown. 

----------------------------------------------------------------------
### Packages and Data
----------------------------------------------------------------------

```{r}
if (!require("pacman")) install.packages("pacman")
pacman::p_load(tictoc, parallel, pbapply, future, future.apply,
    furrr, RhpcBLASctl, memoise, here, ranger, missForest, tidyverse)
plan(multisession)

```


```{r}

#read data in if needed:
#rf1_data <- read.csv("data/imputed_full.csv", header = TRUE)
#str(rf1_data)

#change col names:

names(imputed_full) <- gsub(x = names(imputed_full), pattern = " ", replacement = ".")

# Data Partition
set.seed(123)
ind <- sample(2, nrow(imputed_full), replace = TRUE, prob = c(0.7, 0.3))
train <- imputed_full[ind==1,]
test <- imputed_full[ind==2,]

# Number of features:
n_features <- length(setdiff(names(imputed_full), "Income Change"))


# Random Forest with randomForest package
library(randomForest)
set.seed(123)

#running first rf model, not tuned yet

tic()
rf1 <- randomForest(Income.Change ~ ., 
                   data = train,
                   ntree = 500,
                   mtry = 4,
                   importance = TRUE,
                   proximity = TRUE)
toc()

print(rf1)
attributes(rf1)

# Prediction & Confusion Matrix - train data
library(caret)

p1 <- predict(rf1, train)
confusionMatrix(p1, train$Income.Change)


# Prediction & Confusion Matrix - test data
p2 <- predict(rf1, test)
confusionMatrix(p2, test$Income.Change)

# Error rate of Random Forest
plot(rf1)

# No. of nodes for the trees
hist(treesize(rf1),
     main = "No. of Nodes for the Trees",
     col = "green")

# Variable Importance
varImpPlot(rf1,
           sort = T,
           n.var = 10,
           main = "Top 10 - Variable Importance")

imprf1 <- randomForest::importance(rf1)
varUsed(rf1)

# Partial Dependence Plot
partialPlot(rf1, train, x.var = Income.Change, which.class = "2", plot = TRUE)

# Extract Single Tree
getTree(rf1, 1, labelVar = TRUE)

# Multi-dimensional Scaling Plot of Proximity Matrix

MDSplot(rf1, train$Income.Change)


# ROC and AUC curves:
p_load("pROC")

rf1_roc <- roc(train$Income.Change, rf1$votes[,2])

plot(rf1_roc)
auc(rf1_roc)


# Tune mtry

p_load(parallelMap, parallel)
parallelStartSocket (cpus = detectCores())

tic()
t <- tuneRF(train[,-1], train[,1],
       stepFactor = 0.5,
       plot = TRUE,
       ntreeTry = 1000,
       trace = TRUE,
       improve = 0.05)
toc()

```
----------------------------------------------------------------------
# Running RF again after tuning: Model is called rf3
----------------------------------------------------------------------
```{r}

rf3 <- randomForest(Income.Change ~ ., 
                   data = train,
                   ntree = 700,
                   mtry = 8,
                   importance = TRUE,
                   proximity = TRUE)

print(rf3)
attributes(rf3)

# Prediction & Confusion Matrix - train data

p3 <- predict(rf3, train)
confusionMatrix(p3, train$Income.Change)

# Prediction & Confusion Matrix - test data
p4 <- predict(rf3, test)
confusionMatrix(p4, test$Income.Change)

# Error rate of Random Forest
plot(rf3)

rf3_impplot <- varImpPlot(rf1,
           sort = T,
           n.var = 10,
           main = "Tuned RF - Variable Importance")


imprf3 <- randomForest::importance(rf3)
varUsed(rf3)

# Partial Dependence Plot
partialPlot(rf3, train, x.var = Income.Change, which.class = "2", plot = TRUE)

# Extract Single Tree
getTree(rf3, 1, labelVar = TRUE)

# Multi-dimensional Scaling Plot of Proximity Matrix

MDSrf3train <- MDSplot(rf3, train$Income.Change)

MDSrf3test <- MDSplot(rf3, test$Income.Change)


# ROC and AUC curves:

rf3_roc <- roc(train$Income.Change, rf3$votes[,2])

plot(rf3_roc)
auc(rf3_roc)

```

######################################################################
----------------------------------------------------------------------
### Simple GBM - Classification:
----------------------------------------------------------------------
######################################################################

# Setting Up Grid:

```{r}
p_load("gbm")

grid <- expand.grid(n.trees = seq(200,1000,by=50), interaction.depth = seq(1,5,by=2), shrinkage = seq(.01,.09,by=.04), n.minobsinnode = seq (1,30, by = 5)) # grid features

control <- trainControl(method="CV", number = 10) # control - 10-fold cross-validation
```
----------------------------------------------------------------------
# Grid Search for Simple gradient boosted model:
----------------------------------------------------------------------
finding optimal n.trees = 800, 
interaction.depth = 3, 
shrinkage = 0.01, 
n.minobssinnode = 1.

```{r}
set.seed(123)

tic()
gbm_train <- train(Income.Change~.,data=train, method='gbm', trControl=control, tuneGrid=grid)
toc()

gbm_train 


# this gives optimal values for : n.trees = 300, interaction.depth = 5, shrinkage = 0.05 , n.minobssinnode = 11.
```

----------------------------------------------------------------------
# Training GBM:
----------------------------------------------------------------------

```{r}
#making outcome variable a dummy needed for gbm function

train$Income.Change = ifelse(train$Income.Change == "2", 0 , 1)

set.seed(123)
gbm1 <- gbm(Income.Change~., distribution = 'bernoulli', data=train, n.trees = 300, interaction.depth = 5, shrinkage=.05, n.minobsinnode = 11)

gbm1

```

----------------------------------------------------------------------
# Testing GBM:
----------------------------------------------------------------------
```{r}
gbm1test <- predict(gbm1, newdata = test, type = 'response', n.trees = 300)

gbmclass <-ifelse(gbm1test < 0.5 ,2 , 1)

table(gbmclass,test$Income.Change)

gbm1accuracy <-((495 +986)/(495 +394  +225 + 986)) # does not improve on accuracy really, 0.7052381. It's actually lower than model rf3 (0.7124)

```


######################################################################
----------------------------------------------------------------------
### MODEL 3: XGBOOST Imputed data - Classification:
----------------------------------------------------------------------
######################################################################


```{r}
#data prep:

p_load("recipes", "xgboost")

xgb_prep <- recipe(Income.Change ~ ., data = train) %>%
    step_integer(all_nominal()) %>%
    prep(training = train, retain = TRUE) %>%
    juice()

X <- as.matrix(xgb_prep[setdiff(names(xgb_prep), "Income.Change")])

Y <- xgb_prep$Income.Change
```

----------------------------------------------------------------------
### XGB grid searches
----------------------------------------------------------------------

```{r}
set.seed(123)
xgb1 <- xgb.cv(
    data = X,
    label = Y,
    nrounds = 5000,
    objective = "binary:hinge",
    early_stopping_rounds = 50,
    eval_metric = "error",
    nfold = 10,
    params = list(
        booster="gbtree",
        eta = 0.1,
        max_depth = 3,
        min_child_weight = 3,
        subsample = 0.8,
        colsample_bytree = 1.0),
    verbose = 0
)

min(xgb1$evaluation_log$test_error_mean)

```

Next, we assess if overfitting is limiting our modelâ€™s performance by performing a
grid search that examines various regularisation parameters


```{r}
# hyperparameter grid
hyper_grid <- expand.grid(
    eta = 0.01,
    max_depth = 3,
    min_child_weight = 3,
    subsample = 0.5,
    colsample_bytree = 0.5,
    gamma = c(0, 1, 10, 100, 1000),
    lambda = c(0, 1e-2, 0.1, 1, 100, 1000, 10000),
    alpha = c(0, 1e-2, 0.1, 1, 100, 1000, 10000),
    error = 0, # a place to dump error results
    trees = 0 
)

```

#Grid search continues

```{r}

#set parallel
doParallel::registerDoParallel(cores = 8) # set based on number of CPU cores
doRNG::registerDoRNG(seed = 123)

# grid search
tic()
for(i in seq_len(nrow(hyper_grid))){
  
    set.seed(123)
  
    m <- xgb.cv(data = X,
      label = Y,
      nrounds = 5000,
      objective = "binary:hinge",
      early_stopping_rounds = 50,
      eval_metric = "error",
      nfold = 10,
      verbose = 0,
      params = list(
        booster = "gbtree",
        eta = hyper_grid$eta[i],
        max_depth = hyper_grid$max_depth[i],
        min_child_weight = hyper_grid$min_child_weight[i],
        subsample = hyper_grid$subsample[i],
        colsample_bytree = hyper_grid$colsample_bytree[i],
        gamma = hyper_grid$gamma[i],
        lambda = hyper_grid$lambda[i],
        alpha = hyper_grid$alpha[i]
      )
    )
    
    hyper_grid$error[i] <- min(m$evaluation_log$test_error_mean)
    hyper_grid$trees[i] <- m$best_iteration

}
toc()






```

# Model


```{r}
# optimal parameter list
params <- list(
    eta = 0.01,
    max_depth = 3,
    min_child_weight = 3,
    subsample = 0.5,
    colsample_bytree = 0.5
)

# train final model
xgb.fit.final <- xgboost(
    params = params,
    data = X,
    label = Y,
    nrounds = 3944,
    objective = "binary:hinge",
    eval_metric = "error",
    verbose = 0
)


```




